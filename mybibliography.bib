@inproceedings{TilletPhilippe2019Tail,
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial.
We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
address = {New York, NY, USA},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
copyright = {2019 ACM},
isbn = {9781450367196},
language = {eng},
pages = {10-19},
publisher = {ACM},
title = {Triton: an intermediate language and compiler for tiled neural network computations},
year = {2019},
}
@article{AshouriAmirH2018ASoC,
abstract = {Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches and finally, the influential papers of the field.},
author = {Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Artificial intelligence ; Machine learning},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {A Survey on Compiler Autotuning using Machine Learning},
year = {2018},
}

@article{MohaidatTamador2024ASoN,
abstract = {Artificial intelligence (AI) hardware accelerator is an emerging research for several applications and domains. The hardware accelerator's direction is to provide high computational speed with retaining low-cost and high learning performance. The main challenge is to design complex machine learning models on hardware with high performance. This article presents a thorough investigation into machine learning accelerators and associated challenges. It describes a hardware implementation of different structures such as convolutional neural network (CNN), recurrent neural network (RNN), and artificial neural network (ANN). The challenges such as speed, area, resource consumption, and throughput are discussed. It also presents a comparison between the existing hardware design. Last, the article describes the evaluation parameters for a machine learning accelerator in terms of learning and testing performance and hardware design.},
author = {Mohaidat, Tamador and Khalil, Kasem},
issn = {2691-4581},
journal = {IEEE transactions on artificial intelligence},
keywords = {Artificial intelligence ; Costs ; Machine learning},
language = {eng},
number = {8},
pages = {3801-3822},
publisher = {IEEE},
title = {A Survey on Neural Network Hardware Accelerators},
volume = {5},
year = {2024},
}
@article{MoolchandaniDiksha2021ACIo,
abstract = {Convolutional neural networks (CNNs) have proven to be a disruptive technology in most vision, speech and image processing tasks. Given their ubiquitous acceptance, the research community is investing a lot of time and resources on deep neural networks. Custom hardware such as ASICs are proving to be extremely worthy platforms for running such programs. However, the ever-increasing complexity of these algorithms poses challenges in achieving real-time performance. Specifically, CNNs have prohibitive costs in terms of computation time, throughput, latency, storage space, memory bandwidth, and power consumption.
Hence, in the last 5 years, a lot of work has been done by the scientific community to mitigate these costs. Researchers have primarily focused on reducing the computation time, the number of computations, the memory access time, and the size of the memory footprint. In this survey paper, we propose a novel taxonomy to classify prior work, and describe some of the key contributions in these areas in detail.},
author = {Moolchandani, Diksha and Kumar, Anshul and Sarangi, Smruti R.},
copyright = {2020 Elsevier B.V.},
issn = {1383-7621},
journal = {Journal of systems architecture},
language = {eng},
pages = {101887-},
publisher = {Elsevier B.V},
title = {Accelerating CNN Inference on ASICs: A Survey},
volume = {113},
year = {2021},
}

@article{RicoAlejandro2024AXNi,
abstract = {The AMD Ryzen 7040 series is the first x86 processor with an integrated neural processing unit (NPU). The AMD XDNA technology in the NPU of Ryzen artificial intelligence (AI) processors provides optimized compute and memory resources for the needs for AI workloads and a specialized data movement architecture that allows to minimize bandwidth requirements leading to higher performance and efficiency. Across a selection of neural network benchmarks, XDNA provides between 4.3× and 33× better performance per watt, leading to extended battery life. The AI-optimized capabilities of the Ryzen 7040 NPU enable new AI experiences that are not possible without XDNA, making it a fundamental component in today’s Ryzen-AI-powered devices and setting the foundation for an exciting roadmap toward future AI capabilities in mobile PCs.},
author = {Rico, Alejandro and Pareek, Satyaprakash and Cabezas, Javier and Clarke, David and Ozgul, Baris and Barat, Francisco and Fu, Yao and Munz, Stephan and Stuart, Dylan and Schlangen, Patrick and Duarte, Pedro and Date, Sneha and Paul, Indrani and Weng, Jian and Santan, Sonal and Kathail, Vinod and Sirasao, Ashish and Noguera, Juanjo},
issn = {0272-1732},
journal = {IEEE MICRO},
keywords = {Artificial intelligence ; Business enterprises ; Graphics processing units ; Mobile computing ; Product design},
language = {eng},
number = {6},
pages = {73-82},
publisher = {IEEE},
title = {AMD XDNA NPU in Ryzen AI Processors},
volume = {44},
year = {2024},
}

@article{LuWenbin2022BtB,
abstract = {In this paper, we present a framework for moving compute and data between processing elements in a distributed heterogeneous system. The implementation of the framework is based on the LLVM compiler toolchain combined with the UCX communication framework. The framework can generate binary machine code or LLVM bitcode for multiple CPU architectures and move the code to remote machines while dynamically optimizing and linking the code on the target platform. The remotely injected code can recursively propagate itself to other remote machines or generate new code. The goal of this paper is threefold: (a) to present an architecture and implementation of the framework that provides essential infrastructure to program a new class of disaggregated systems wherein heterogeneous programming elements such as compute nodes and data processing units (DPUs) are distributed across the system, (b) to demonstrate how the framework can be integrated with modern, high-level programming languages such as Julia, and (c) to demonstrate and evaluate a new class of eXtended Remote Direct Memory Access (X-RDMA) communication operations that are enabled by this framework. To evaluate the capabilities of the framework, we used a cluster with Fujitsu CPUs and heterogeneous cluster with Intel CPUs and BlueField-2 DPUs interconnected using high-performance RDMA fabric. We demonstrated an X-RDMA pointer chase application that outperforms an RDMA GET-based implementation by 70% and is as fast as Active Messages, but does not require function predeployment on remote platforms.},
author = {Lu, Wenbin and Peña, Luis E and Shamis, Pavel and Churavy, Valentin and Chapman, Barbara and Poole, Steve},
address = {Ithaca},
copyright = {2022. This work is published under http://creativecommons.org/licenses/by-nc-nd/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Data processing},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Bring the BitCODE -- Moving Compute and Data in Distributed Heterogeneous Systems},
year = {2022},
}
@article{LieSean2023CADD,
abstract = {The compute and memory demands for deep learning and machine learning (ML) have increased by several orders of magnitude in just the last couple of years, and there is no end in sight. Traditional improvements in processor performance alone struggle to keep up with the exponential demand. A new chip architecture co-designed with the ML algorithms can be better equipped to satisfy this unprecedented demand and enable the ML workloads of the future. This article describes the Cerebras architecture and how it is designed specifically with this purpose, from the ground up, as a wafer-sized chip to enable emerging extreme-scale ML models. It uses fine-grained data flow compute cores to accelerate unstructured sparsity, distributed static random-access memory for full memory bandwidth to the data paths, and a specially designed on-chip and off-chip interconnect for ML training. With these techniques, the Cerebras architecture provides unique capabilities beyond traditional designs.},
author = {Lie, Sean},
address = {Los Alamitos},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2023},
issn = {0272-1732},
journal = {IEEE MICRO},
keywords = {Algorithms ; Computer architecture ; Distributed databases ; Hardware ; Machine learning ; Microcomputers ; Microprocessors ; Product design ; Software ; Static random access memory},
language = {eng},
number = {3},
pages = {18-30},
publisher = {IEEE},
title = {Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning},
volume = {43},
year = {2023},
}
@article{ZhangHongbin2023CTiD,
abstract = {With the rapid development of deep learning applications, general-purpose processors no longer suffice for deep learning workloads because of the dying of Moore’s Law. Thus, computer architecture innovation has entered a golden age for domain-specific design, which has led to a demand for new compilation technologies to facilitate cross-layer optimization. Historically, hardware and software have been collaboratively designed. Today, these co-design ideas still benefit the deep learning field in both academia and industry, encompassing additional aspects and layers. In this study, we elaborate on past and recent works on deep learning compilers and co-design while focusing on the combination of these two technologies, which we believe is the trend in the new deep learning era. After summarizing the existing compilation technologies and co-design approaches, we propose a domain-specific compilation framework, the Buddy Compiler, for a typical deep learning co-design system.},
author = {Zhang, Hongbin and Xing, Mingjie and Wu, Yanjun and Zhao, Chen},
issn = {2771-5892},
journal = {Intelligent computing},
language = {eng},
publisher = {American Association for the Advancement of Science (AAAS)},
title = {Compiler Technologies in Deep Learning Co-Design: A Survey},
volume = {2},
year = {2023},
}
@article{LeppänenTopi2022Cpaf,
abstract = {Hardware specialization is a well-known means to significantly improve the performance and energy efficiency of various application domains. Modern computing systems consist of multiple specialized processing devices which need to collaborate with each other to execute common tasks. New heterogeneous programming abstractions have been created to program heterogeneous systems. Even though many of these abstractions are open vendor-independent standards, cross-vendor interoperability between different implementations is limited since the vendors typically do not have commercial motivations to invest in it. Therefore, getting good performance from vendor-independent heterogeneous programming standards has proven difficult for systems with multiple different device types. In order to help unify the field of heterogeneous programming APIs for platforms with hardware accelerators from multiple vendors, we propose a new software abstraction for hardware-accelerated tasks based on the open OpenCL programming standard. In the proposed abstraction, we rely on the built-in kernel feature of the OpenCL specification to define a portability layer that stores enough information for automated accelerator utilization. This enables the portability of high-level applications to a diverse set of accelerator devices with minimal programmer effort. The abstraction enables a layered software architecture that provides for an efficient combination of application phases to a single asynchronous application description from multiple domain-specific input languages. As proofs of the abstraction layer serving its purpose for the layers above and below it, we show how a domain-specific input description ONNX can be implemented on top of this portability abstraction, and how it also allows driving fixed function and FPGA-based hardware accelerators below in the hardware-specific backend. We also provide an example implementation of the abstraction to show that the abstraction layer does not seem to incur significant execution time overhead.},
author = {Leppänen, Topi and Lotvonen, Atro and Jääskeläinen, Pekka},
issn = {2624-9898},
journal = {Frontiers in computer science (Lausanne)},
keywords = {Heterogeneous computing},
language = {eng},
publisher = {Frontiers Media S.A},
title = {Cross-vendor programming abstraction for diverse heterogeneous platforms},
volume = {4},
year = {2022},
}

@article{EjjehAdel2022HHPf,
abstract = {We present Heterogeneous Parallel Virtual Machine (HPVM), a compiler framework for hardware-agnostic programming on heterogeneous compute platforms. HPVM introduces a hardware-agnostic parallel intermediate representation with constructs for the hierarchical task, data, and pipeline parallelism, including dataflow parallelism, and supports multiple front-end languages. In addition, HPVM provides optimization passes that navigate performance, energy, and accuracy tradeoffs, and includes retargetable back ends for a wide range of diverse hardware targets, including central processing units, graphics processing units, domain-specific accelerators, and field-programmable gate arrays. Across diverse hardware platforms, HPVM optimizations provide significant performance and energy improvements, while preserving object-code portability. With these capabilities, HPVM facilitates developers, domain experts, and hardware vendors in programming modern heterogeneous systems.},
author = {Ejjeh, Adel and Councilman, Aaron and Kothari, Akash and Kotsifakou, Maria and Medvinsky, Leon and Noor, Abdul Rafae and Sharif, Hashim and Zhao, Yifan and Adve, Sarita and Misailovic, Sasa and Adve, Vikram},
address = {Los Alamitos},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2022},
issn = {0272-1732},
journal = {IEEE MICRO},
keywords = {Ciphers ; Field programmable gate arrays ; Graphics processing units ; Hardware ; Platforms ; Programming ; Tuning ; Virtual reality},
language = {eng},
number = {5},
pages = {108-117},
publisher = {IEEE},
title = {HPVM: Hardware-Agnostic Programming for Heterogeneous Parallel Systems},
volume = {42},
year = {2022},
}

@article{JosseVanDelm2024HENN,
abstract = {Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge. The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency. We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements. HTVM allows deploying the MLPerf(TM) Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment.},
author = {Josse Van Delm and Vandersteegen, Maarten and Burrello, Alessio and Sarda, Giuseppe Maria and Conti, Francesco and Pagliari, Daniele Jahier and Benini, Luca and Verhelst, Marian},
address = {Ithaca},
copyright = {2024. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Machine learning ; Neural networks (Computer science)},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {HTVM: Efficient Neural Network Deployment On Heterogeneous TinyML Platforms},
year = {2024},
}

%works 
@article{OneAPICloud,
author = {Alcaraz, Silvia and Laso, Ruben and Lorenzo, Oscar and Vilariño, David and Pena, Tomas and Rivera, Francisco},
year = {2024},
month = {03},
pages = {1-22},
title = {Assessing Intel OneAPI capabilities and cloud-performance for heterogeneous computing},
volume = {80},
journal = {The Journal of Supercomputing},
doi = {10.1007/s11227-024-05958-5}
}

@inproceedings{ONNXFailures,
author = {Jajal, Purvish and Jiang, Wenxin and Tewari, Arav and Kocinare, Erik and Woo, Joseph and Sarraf, Anusha and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
title = {Interoperability in Deep Learning: A User Survey and Failure Analysis of ONNX Model Converters},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680374},
doi = {10.1145/3650212.3680374},
abstract = {Software engineers develop, fine-tune, and deploy deep learning (DL) models using a variety of development frameworks and runtime environments.                                                                DL model converters move models between frameworks and to runtime environments.                                                                Conversion errors compromise model quality and disrupt deployment.                                                                However, the failure characteristics of DL model converters are unknown, adding risk when using DL interoperability technologies.                                                                                                                                This paper analyzes failures in DL model converters.                                                                We survey software engineers about DL interoperability tools, use cases, and pain points (N=92).                                                                Then, we characterize failures in model converters associated with the main interoperability tool, ONNX (N=200 issues in PyTorch and TensorFlow).                                                                Finally, we formulate and test two hypotheses about structural causes for the failures we studied.                                                                We find that the node conversion stage of a model converter accounts for ∼75\% of the defects and 33\% of reported failure are related to semantically incorrect models.                                                                The cause of semantically incorrect models is elusive, but models with behaviour inconsistencies share operator sequences.                                                                Our results motivate future research on making DL interoperability software simpler to maintain, extend, and validate.                                                                Research into behavioural tolerances and architectural coverage metrics would be fruitful.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1466–1478},
numpages = {13},
keywords = {Deep neural networks, Interoperabilty Empirical software engineering, Machine learning, ONNX},
location = {Vienna, Austria},
series = {ISSTA 2024}
}
@inproceedings{DemeureNestor2023HGca,
abstract = {In recent years, a new scientific software design pattern has emerged, pairing a Python interface with high-performance kernels in lower-level languages. The rise of general-purpose GPUs necessitates the rewriting of many such kernels, which poses challenges in GPU programming and ensures future portability and flexibility. This paper documents our experience and observations during the process of porting TOAST, a cosmology software framework designed to take full advantage of a supercomputer, to work with GPUs. This exploration led us to compare two different porting strategies: utilizing the JAX Python library and employing OpenMP Target Offload compiler directives. JAX allows kernel code to be written in pure Python, whereas OpenMP Target Offload is a directive-based strategy that integrates seamlessly with our existing OpenMP-accelerated C++ kernels. Both frameworks are high-level, abstracting system architecture details while aiming for straightforward, portable, yet performant GPU code. Through the porting of a dozen kernels, we delve into the analysis of development cost, performance, and the viability of employing either of these frameworks for complex numerical Python applications.},
author = {Demeure, Nestor and Kisner, Theodore and Keskitalo, Reijo and Thomas, Rollin and Borrill, Julian and Bhimji, Wahid},
address = {New York, NY, USA},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
copyright = {2023 Owner/Author},
isbn = {9798400707858},
language = {eng},
pages = {1105-1113},
publisher = {ACM},
title = {High-level GPU code: a case study examining JAX and OpenMP},
year = {2023},
}

@inproceedings{LattnerChris2021Msci,
abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together.
MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers---describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
address = {Piscataway, NJ, USA},
booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
isbn = {1728186137},
keywords = {Buildings ; Hardware ; Semantics ; Software},
language = {eng},
pages = {2-14},
publisher = {IEEE Press},
title = {MLIR: scaling compiler infrastructure for domain specific computation},
year = {2021},
}
@article{MicikeviciusPaulius2018MPT,
abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Machine learning ; Mathematical models ; Training},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Mixed Precision Training},
year = {2018},
}


@inproceedings{MarkidisStefano2018NTCP,
abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
booktitle = {2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
isbn = {9781538655559},
keywords = {Computer architecture ; Graphics processing units ; Hardware ; Programming},
language = {eng},
pages = {522-531},
publisher = {IEEE},
title = {NVIDIA Tensor Core Programmability, Performance & Precision},
year = {2018},
}

@inproceedings{ONNC,
abstract = {This paper presents ONNC (Open Neural Network Compiler), a retargetable compilation framework designed to connect ONNX (Open Neural Network Exchange) models to proprietary deep learning accelerators (DLAs). The intermediate representations (IRs) of ONNC have one-to-one mapping to ONNX IRs, thus making porting ONNC to proprietary DLAs much simpler than other compilation frameworks such as TVM and Glow especially for hardware with coarse-grained operators that are not part of the generic IRs in the LLVM backend. ONNC also has a flexible pass manager designed to support compiler optimizations at all levels. A docker image of ONNC bundled with a Vanilla backend is released with this paper to enable fast porting to new hardware targets. To illustrate how an ONNC-based toolkit guides our research and development in DLA design, we present a case study on compiler optimizations for activation memory consumption. The study shows that the Best-Fit algorithm with a proposed heuristic and a reordering scheme may act as a near-optimal strategy, getting the memory consumption close to the ideal lower bound in 11 of 12 models from the ONNX model zoo. To our best knowledge, ONNC is the first open source compilation framework that is specially designed to support the ONNX-based models for both commercial and research projects for deep learning applications.},
author = {Lin, Wei-Fen and Tsai, Der-Yu and Tang, Luba and Hsieh, Cheng-Tao and Chou, Cheng-Yi and Chang, Ping-Hao and Hsu, Luis},
booktitle = {2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)},
isbn = {1538678845},
keywords = {Hardware ; Software},
language = {eng},
pages = {214-218},
publisher = {IEEE},
title = {ONNC: A Compilation Framework Connecting ONNX to Proprietary Deep Learning Accelerators},
year = {2019},
}

@article{ONNXFlow,
abstract = {The challenges involved in executing neural networks (NNs) at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the NNs, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work, starting from a preliminary semi-integrated ONNX-to-hardware toolchain [21], focuses on enabling approximate computing leveraging the distinctive ability of the original toolchain to favor adaptivity. The goal is to allow lightweight adaptable NN inference on FPGAs at the edge.},
author = {Manca, Federico and Ratto, Francesco and Palumbo, Francesca},
address = {Ithaca},
copyright = {2024. This work is published under http://creativecommons.org/licenses/by-nc-sa/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Algorithms ; Field programmable gate arrays ; Hardware ; Inference},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {ONNX-to-Hardware Design Flow for Adaptive Neural-Network Inference on FPGAs},
year = {2024},
}
@article{OpenCLLaunch,
abstract = {We provide an overview of the key architectural features of recent microprocessor designs and describe the programming model and abstractions provided by OpenCL, a new parallel programming standard targeting these architectures.},
author = {Stone, John E. and Gohara, David and Shi, Guochun},
address = {United States},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) May/Jun 2010},
issn = {1521-9615},
journal = {Computing in science & engineering},
keywords = {Computer architecture ; Computer interfaces ; Hardware ; High performance computing ; Microcomputers ; Microprocessors},
language = {eng},
number = {3},
pages = {66-73},
publisher = {IEEE},
title = {OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems},
volume = {12},
year = {2010},
}

@article{QPU,
abstract = {The integration of quantum processing units (QPUs) in a heterogeneous high-performance computing environment requires solutions that facilitate hybrid classical–quantum programming. Standards such as OpenCL facilitate the programming of heterogeneous environments, consisting of CPUs and hardware accelerators. This study presents an innovative method that incorporates QPU functionality into OpenCL, standardizing quantum processes within classical environments. By leveraging QPUs within OpenCL, hybrid quantum–classical computations can be sped up, impacting domains like cryptography, optimization problems, and quantum chemistry simulations. Using Portable Computing Language (Jääskeläinen et al. in Int J Parallel Program 43(5):752–785, 2014.
https://doi.org/10.1007/s10766-014-0320-y
) and the Qulacs library (Suzuki et al. in Quantum 5:559, 2021.
https://doi.org/10.22331/q-2021-10-06-559
), results demonstrate, for instance, the successful execution of Shor’s algorithm (Nielsen and Chuang in Quantum computation and quantum information, 10th anniversary edn. Cambridge University Press, Cambridge, 2010), serving as a proof of concept for extending the approach to larger qubit systems and other hybrid quantum–classical algorithms. This integration approach bridges the gap between quantum and classical computing paradigms, paving the way for further optimization and application to a wide range of computational problems.},
author = {Vázquez-Pérez, Jorge and Piñeiro, César and Pichel, Juan C. and Pena, Tomás F. and Gómez, Andrés},
address = {New York},
copyright = {The Author(s) 2024},
issn = {0920-8542},
journal = {The Journal of supercomputing},
keywords = {Algorithms ; Computer science ; Cryptography ; Hybrid systems ; Quantum chemistry ; Quantum computing ; Translators},
language = {eng},
number = {8},
pages = {11682-11703},
publisher = {Springer US},
title = {QPU integration in OpenCL for heterogeneous programming},
volume = {80},
year = {2024},
}

@article{SYCLEdge,
abstract = {Edge computing is essential to handle increasing data volumes and processing capacities. It provides real-time and secure data processing near data sources, like smart devices, alleviating cloud computing energy use, and saving network bandwidth. Specialized accelerators, like GPUs and FPGAs, are vital for low-latency edge computing but the requirements to customized code for different hardware and vendors suppose important compatibility issues. This paper evaluates the potential of SYCL in addressing code portability issues encountered in edge computing. We employed the Polybench suite to compare various SYCL implementations, specifically DPC++ and AdaptiveCpp, with the native solution, CUDA. The disparity between SYCL implementations was negligible, at just 5%. Furthermore, we evaluated SYCL in the context of specific edge computing applications such as video processing using three different optical flow algorithms. The results revealed a slight performance gap of 3% when transitioning from CUDA to SYCL. Upon evaluating energy consumption, the observed difference ranged from
±
10
%
, depending on the application utilized. These gaps are the price one may need to pay when achieving the ability to successfully run the same code on two distinct edge boards. These findings underscore SYCL’s capacity to increase productivity in terms of development costs and facilitate IoT deployment without being locked into a particular platform or manufacturer.},
author = {Youssef, Faqir-Rhazoui and García, Carlos},
address = {New York},
copyright = {The Author(s) 2024},
issn = {0920-8542},
journal = {The Journal of supercomputing},
keywords = {Algorithms ; Cloud computing ; Computer science ; Data processing ; Edge computing ; Energy consumption ; Image processing ; Translators},
language = {eng},
number = {10},
pages = {14203-14223},
publisher = {Springer US},
title = {SYCL in the edge: performance and energy evaluation for heterogeneous acceleration},
volume = {80},
year = {2024},
}

@article{TheHardwareLottery,
abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which make it increasingly costly to stray off of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further obstructed.},
author = {Hooker, Sara},
address = {Ithaca},
copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Algorithms ; Hardware ; Lotteries ; Software},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {The Hardware Lottery},
year = {2020},
}
@inproceedings{NiemierM.2024SDAt,
abstract = {Multiple research vectors represent possible paths to improved energy and performance metrics at the application-level. There are active efforts with respect to emerging logic devices, new memory technologies, novel interconnects, and heterogeneous integration architectures. Of great interest is quantifying the potential impact of a given solution to prioritize research vectors accordingly. In this paper, we discuss two efforts - one focused on emerging memory technology, and another focused on heterogeneous integration technology - that speak to best practices for, and needed contributions from the design automation (DA) community to explore this vast design space. Furthermore, we highlight new research efforts that aim to develop the novel compiler abstractions and frameworks that are ultimately needed to derive maximum value from new memory and/or heterogeneous and monolithic integration architecture, and that can also play an important role with respect to design space exploration efforts.},
author = {Niemier, M. and Enciso, Z. and Sharifi, M. and Hu, X.S. and O'Connor, Ian and Graening, A. and Sharma, R. and Gupta, P. and Castrillon, J. and Lima, J.P.C. and Khan, A.A. and Farzaneh, H. and Afroze, N. and Khan, A. and Ryckaert, Julien},
booktitle = {2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)},
isbn = {3981926382},
issn = {1558-1101},
keywords = {Computer architecture ; Measurement},
language = {eng},
pages = {1-10},
publisher = {EDAA},
title = {Smoothing Disruption Across the Stack: Tales of Memory, Heterogeneity, & Compilers},
year = {2024},
}


@article{DeepLearningCompilerSurvey,
abstract = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},
author = {Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
address = {New York},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2021},
issn = {1045-9219},
journal = {IEEE transactions on parallel and distributed systems},
keywords = {Computer architecture ; Hardware ; Libraries ; Mathematical optimization},
language = {eng},
number = {3},
pages = {708-727},
publisher = {IEEE},
title = {The Deep Learning Compiler: A Comprehensive Survey},
volume = {32},
year = {2021},
}

%works
@INPROCEEDINGS{Groq,
  author={Abts, Dennis and Kim, John and Kimmell, Garrin and Boyd, Matthew and Kang, Kris and Parmar, Sahil and Ling, Andrew and Bitar, Andrew and Ahmed, Ibrahim and Ross, Jonathan},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)}, 
  title={The Groq Software-defined Scale-out Tensor Streaming Multiprocessor : From chips-to-systems architectural overview}, 
  year={2022},
  volume={},
  number={},
  pages={1-69},
  keywords={Tensors},
  doi={10.1109/HCS55958.2022.9895630}
}

@inproceedings{ThinkFast,
abstract = {In this paper, we introduce the Tensor Streaming Processor (TSP) architecture, a functionally-sliced microarchitecture with memory units interleaved with vector and matrix deep learning functional units in order to take advantage of dataflow locality of deep learning operations. The TSP is built based on two key observations: (1) machine learning workloads exhibit abundant data parallelism, which can be readily mapped to tensors in hardware, and (2) a simple and deterministic processor with producer-consumer stream programming model enables precise reasoning and control of hardware components, achieving good performance and power efficiency. The TSP is designed to exploit parallelism inherent in machine-learning workloads including instruction-level, memory concurrency, data and model parallelism, while guaranteeing determinism by eliminating all reactive elements in the hardware (e.g. arbiters, and caches). Early ResNet50 image classification results demonstrate 20.4K processed images per second (IPS) with a batch-size of one---a 4x improvement compared to other modern GPUs and accelerators [44]. Our first ASIC implementation of the TSP architecture yields a computational density of more than 1 TeraOp/s per square mm of silicon for its 25x29 mm 14nm chip operating at a nominal clock frequency of 900 MHz. The TSP demonstrates a novel hardware-software approach to achieve fast, yet predictable, performance on machine-learning workloads within a desired power envelope.},
author = {Abts, Dennis and Ross, Jonathan and Sparling, Jonathan and Wong-VanHaren, Mark and Baker, Max and Hawkins, Tom and Bell, Andrew and Thompson, John and Kahsai, Temesghen and Kimmell, Garrin and Hwang, Jennifer and Leslie-Hurd, Rebekah and Bye, Michael and Creswick, E. R. and Boyd, Matthew and Venigalla, Mahitha and Laforge, Evan and Purdy, Jon and Kamath, Purushotham and Maheshwari, Dinesh and Beidler, Michael and Rosseel, Geert and Ahmad, Omar and Gagarin, Gleb and Czekalski, Richard and Rane, Ashay and Parmar, Sahil and Werner, Jeff and Sproch, Jim and Macias, Adrian and Kurtz, Brian},
address = {Piscataway, NJ, USA},
booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
isbn = {1728146615},
language = {eng},
pages = {145-158},
publisher = {IEEE Press},
title = {Think fast: a tensor streaming processor (TSP) for accelerating deep learning workloads},
year = {2020},
}

@inproceedings{Triton,
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial.
We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
address = {New York, NY, USA},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
copyright = {2019 ACM},
isbn = {9781450367196},
language = {eng},
pages = {10-19},
publisher = {ACM},
title = {Triton: an intermediate language and compiler for tiled neural network computations},
year = {2019},
}

@article{ChenTianqi2018TAAE,
abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Automation ; Electronic apparatus and appliances ; Hardware ; Machine learning},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
year = {2018},
}

@article{XLAArticle,
abstract = {The recent dramatic progress in machine learning is partially attributed to the availability of high-performant computers and development tools. The accelerated linear algebra (XLA) compiler is one such tool that automatically optimises array operations (mostly fusion to reduce memory operations) and compiles the optimised operations into high-performant programs specific to target computing platforms. Like machine-learning models, numerical models are often expressed in array operations, and thus their performance can be boosted by XLA. This study is the first of its kind to examine the efficiency of XLA for numerical models, and the efficiency is examined stringently by comparing its performance with that of optimal implementations. Two shared-memory computing platforms are examined-the CPU platform and the GPU platform. To obtain optimal implementations, the computing speed and its optimisation are rigorously studied by considering different workloads and the corresponding computer performance. Two simple equations are found to faithfully modell the computing speed of numerical models with very few easily-measureable parameters. Regarding operation optimisation within XLA, results show that models expressed in low-level operations (e.g., slice, concatenation, and arithmetic operations) are successfully fused while high-level operations (e.g., convolution and roll) are not. Regarding compilation within XLA, results show that for the CPU platform of certain computers and certain simple numerical models on the GPU platform, XLA achieves high efficiency (> 80%) for large problems and acceptable efficiency (10%~80%) for medium-size problems-the gap is from the overhead cost of Python. Unsatisfactory performance is found for the CPU platform of other computers (operations are compiled in a non-optimal way) and for high-dimensional complex models for the GPU platform, where each GPU thread in XLA handles 4 (single precision) or 2 (double precision) output elements-hoping to exploit the high-performant instructions that can read/write 4 or 2 floating-point numbers with one instruction. However, these instructions are rarely used in the generated code for complex models and performance is negatively affected. Therefore, flags should be added to control the compilation for these non-optimal scenarios.},
author = {He, Xuzhen},
address = {United States},
copyright = {Copyright: © 2023 Xuzhen He. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algebra ; Algebras Linear ; Algorithms ; Computational linguistics ; Computer graphics ; Computers ; Differential equations Partial ; Efficiency ; Machine learning ; Mathematical models ; Mathematics ; Microcomputers ; Microprocessors ; Modeling ; Physical sciences ; Platforms ; Software},
language = {eng},
number = {2},
pages = {e0282265-e0282265},
publisher = {Public Library of Science},
title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
volume = {18},
year = {2023},
}
@article{ShahJay2024FFaA,
abstract = {Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0\(\times\) with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6\(\times\) lower numerical error than a baseline FP8 attention.},
author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
address = {Ithaca},
copyright = {2024. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Graphics processing units ; Hardware},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
year = {2024},
}


@article{FlashAttention,
abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\(\times\) speedup on GPT-2 (seq. length 1K), and 2.4\(\times\) speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).},
author = {Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Atri Rudra and Ré, Christopher},
address = {Ithaca},
copyright = {2022. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Algorithms ; Attention ; Static random access memory},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
year = {2022},
}
@article{RenJie2021ZDBM,
abstract = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency. ZeRO-Offload enables large model training by offloading data and compute to CPU. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU, and reduce CPU compute time while maximizing memory savings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single NVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone for a 1.4B parameter model, the largest that can be trained without running out of memory. ZeRO-Offload is also designed to scale on multiple-GPUs when available, offering near linear speedup on up to 128 GPUs. Additionally, it can work together with model parallelism to train models with over 70 billion parameters on a single DGX-2 box, a 4.5x increase in model size compared to using model parallelism alone. By combining compute and memory efficiency with ease-of-use, ZeRO-Offload democratizes large-scale model training making it accessible to even data scientists with access to just a single GPU.},
author = {Ren, Jie and Rajbhandari, Samyam and Reza Yazdani Aminabadi and Olatunji Ruwase and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
address = {Ithaca},
copyright = {2021. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Efficiency ; Graphics processing units ; Mathematical models ; Scientists ; Training},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
year = {2021},
}
@inproceedings{RajbhandariSamyam2021ZbtG,
abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.
In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 1.},
author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
address = {New York, NY, USA},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
copyright = {2021 ACM},
isbn = {9781450384421},
issn = {2167-4337},
keywords = {Graphics processing units ; High performance computing ; Training},
language = {eng},
pages = {1-14},
publisher = {ACM},
title = {ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning},
year = {2021},
}
@article{10.1145/3571155,
author = {Rathi, Nitin and Chakraborty, Indranil and Kosta, Adarsh and Sengupta, Abhronil and Ankit, Aayush and Panda, Priyadarshini and Roy, Kaushik},
title = {Exploring Neuromorphic Computing Based on Spiking Neural Networks: Algorithms to Hardware},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571155},
doi = {10.1145/3571155},
abstract = {Neuromorphic Computing, a concept pioneered in the late 1980s, is receiving a lot of attention lately due to its promise of reducing the computational energy, latency, as well as learning complexity in artificial neural networks. Taking inspiration from neuroscience, this interdisciplinary field performs a multi-stack optimization across devices, circuits, and algorithms by providing an end-to-end approach to achieving brain-like efficiency in machine intelligence. On one side, neuromorphic computing introduces a new algorithmic paradigm, known as Spiking Neural Networks (SNNs), which is a significant shift from standard deep learning and transmits information as spikes&nbsp;(“1” or “0”) rather than analog values. This has opened up novel algorithmic research directions to formulate methods to represent data in spike-trains, develop neuron models that can process information over time, design learning algorithms for event-driven dynamical systems, and engineer network architectures amenable to sparse, asynchronous, event-driven computing to achieve lower power consumption. On the other side, a parallel research thrust focuses on development of efficient computing platforms for new algorithms. Standard accelerators that are amenable to deep learning workloads are not particularly suitable to handle processing across multiple timesteps efficiently. To that effect, researchers have designed neuromorphic hardware that rely on event-driven sparse computations as well as efficient matrix operations. While most large-scale neuromorphic systems have been explored based on CMOS technology, recently, Non-Volatile Memory (NVM) technologies show promise toward implementing bio-mimetic functionalities on single devices. In this article, we outline several strides that neuromorphic computing based on spiking neural networks (SNNs) has taken over the recent past, and we present our outlook on the challenges that this field needs to overcome to make the bio-plausibility route a successful one.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {243},
numpages = {49},
keywords = {asynchronous communication, Non-Volatile Memories, In-Memory Computing, event cameras, spike-based backpropagation, bio-plausible learning, Spiking Neural Networks, Neuromorphic Computing}
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@online{Modular1,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 1: DeepSeek’s Impact on AI},
url ={https://www.modular.com/blog/democratizing-compute-part-1-deepseeks-impact-on-ai},
month ={jan},
lastaccessed ={April 20, 2025},
}

@online{Modular2,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 2: What exactly is “CUDA”?},
url ={https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda},
month ={feb},
lastaccessed ={April 20, 2025},
}

@online{Modular3,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 3: How did CUDA succeed?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed},
month ={feb},
lastaccessed ={April 20, 2025},
}

@online{Modular4,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 4: CUDA is the incumbent, but is it any good?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good},
month ={feb},
lastaccessed ={April 20, 2025},
}

@online{Modular5,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 5: What about CUDA C++ alternatives like OpenCL?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives},
month ={mar},
lastaccessed ={April 20, 2025},
}

@online{Modular6,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 6: What about AI compilers (TVM and XLA)?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers},
month ={mar},
lastaccessed ={April 20, 2025},
}

@online{Modular7,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 7: What about Triton and Python eDSLs?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls},
month ={mar},
lastaccessed ={April 20, 2025},
}

@online{Modular8,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 8: What about the MLIR compiler infrastructure?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure},
month ={apr},
lastaccessed ={April 20, 2025},
}

@online{Modular9,
author ={Chris Lattner},
year = {2025},
title ={Democratizing AI Compute, Part 9: Why Do HW Companies Struggle to Build AI Software?},
url ={https://www.modular.com/blog/democratizing-ai-compute-part-9-why-do-hw-companies-struggle-to-build-ai-software},
month ={apr},
lastaccessed ={April 20, 2025},
}

@online{NvidiaDominanceYahoo,
author={Nauman khan},
year={2024},
title={NVIDIA Crushes Rivals: Secures Unprecedented 90% of GPU Market in Q3 2024},
url={https://finance.yahoo.com/news/nvidia-crushes-rivals-secures-unprecedented-102235255.html},
month={dec},
lastaccessed={April 20, 2025}
}

@online{maxgpu,
author ={Max Hutchinson, Tyler Kenney},
year = {2024},
title ={MAX GPU: State of the Art Throughput on a New GenAI platform},
url ={https://www.modular.com/blog/max-gpu-state-of-the-art-throughput-on-a-new-genai-platform},
month ={dec},
lastaccessed ={April 20, 2025},
}
